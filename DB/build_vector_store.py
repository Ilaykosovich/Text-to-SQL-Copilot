from __future__ import annotations
import re
import uuid
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer


# ---------- Parsing helpers ----------

SECTION_TITLE_RE = re.compile(r"^[A-Z0-9_]+$")  # e.g. TABLES, COLUMNS, TABLE_COMMENTS

from transformers import AutoTokenizer


def count_tokens_txt(
    file_path: str,
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
) -> int:
    """
    Counts tokens in a TXT file using the tokenizer of the given model.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    tokens = tokenizer.encode(text, add_special_tokens=False)
    return len(tokens)





@dataclass
class Section:
    name: str
    columns: List[str]
    rows: List[List[str]]


def parse_description_txt(path: str) -> Dict[str, Section]:
    """
    Parses the TXT generated by our exporter.

    Expected structure per section:
    ================================================================================
    SECTION_NAME
    ================================================================================
    Columns: col1, col2, col3
    Rows: N
    --------------------------------------------------------------------------------
    <tab-separated rows>
    """
    with open(path, "r", encoding="utf-8") as f:
        lines = [ln.rstrip("\n") for ln in f]

    sections: Dict[str, Section] = {}
    i = 0
    n = len(lines)

    def skip_while(pred):
        nonlocal i
        while i < n and pred(lines[i]):
            i += 1

    while i < n:
        # Find a section header: line of '=' then a title then line of '='
        if lines[i].startswith("=" * 10):
            if i + 2 < n and SECTION_TITLE_RE.match(lines[i + 1].strip()) and lines[i + 2].startswith("=" * 10):
                section_name = lines[i + 1].strip().lower()
                i += 3

                # Find "Columns: ..."
                skip_while(lambda s: s.strip() == "")
                if i >= n or not lines[i].startswith("Columns:"):
                    # Not a valid section, continue scanning
                    continue
                cols_line = lines[i]
                cols = [c.strip() for c in cols_line.replace("Columns:", "").split(",")]
                i += 1

                # Skip "Rows:" line and dashed line
                if i < n and lines[i].startswith("Rows:"):
                    i += 1
                if i < n and lines[i].startswith("-" * 10):
                    i += 1

                # Read tab-separated rows until blank line or next section divider
                rows: List[List[str]] = []
                while i < n and not lines[i].startswith("=" * 10):
                    ln = lines[i].strip()
                    if ln == "":
                        i += 1
                        break
                    parts = ln.split("\t")
                    rows.append(parts)
                    i += 1

                sections[section_name] = Section(name=section_name, columns=cols, rows=rows)
                continue

        i += 1

    return sections


# ---------- Chunk building ----------

def safe_get(row: List[str], idx: int) -> str:
    return row[idx] if idx < len(row) else ""


def build_chunks(sections: Dict[str, Section]) -> List[Tuple[str, Dict[str, str]]]:
    """
    Returns list of (text, metadata) tuples.
    Creates:
      - table_summary chunks
      - column chunks (per column)
      - table_comment chunks (per table)
      - column_comment chunks (per column)
      - fk chunks (per relationship)
    """
    chunks: List[Tuple[str, Dict[str, str]]] = []

    # Index some sections for easier lookup
    tables = sections.get("tables")
    columns = sections.get("columns")
    table_comments = sections.get("table_comments")
    column_comments = sections.get("column_comments")
    foreign_keys = sections.get("foreign_keys")

    # Build lookup maps
    table_desc: Dict[Tuple[str, str], str] = {}
    if table_comments:
        # expected columns: schema_name, table_name, table_description
        colmap = {name: idx for idx, name in enumerate(table_comments.columns)}
        for r in table_comments.rows:
            schema = safe_get(r, colmap.get("schema_name", 0))
            table = safe_get(r, colmap.get("table_name", 1))
            desc = safe_get(r, colmap.get("table_description", 2))
            table_desc[(schema, table)] = desc

    col_desc: Dict[Tuple[str, str, str], str] = {}
    if column_comments:
        colmap = {name: idx for idx, name in enumerate(column_comments.columns)}
        for r in column_comments.rows:
            schema = safe_get(r, colmap.get("schema_name", 0))
            table = safe_get(r, colmap.get("table_name", 1))
            col = safe_get(r, colmap.get("column_name", 2))
            desc = safe_get(r, colmap.get("column_description", 3))
            col_desc[(schema, table, col)] = desc

    cols_by_table: Dict[Tuple[str, str], List[Dict[str, str]]] = {}
    if columns:
        colmap = {name: idx for idx, name in enumerate(columns.columns)}
        for r in columns.rows:
            schema = safe_get(r, colmap.get("table_schema", 0))
            table = safe_get(r, colmap.get("table_name", 1))
            col = safe_get(r, colmap.get("column_name", 3))
            dtype = safe_get(r, colmap.get("data_type", 4))
            nullable = safe_get(r, colmap.get("is_nullable", 5))
            default = safe_get(r, colmap.get("column_default", 6))
            desc = col_desc.get((schema, table, col), "")
            cols_by_table.setdefault((schema, table), []).append({
                "column_name": col,
                "data_type": dtype,
                "is_nullable": nullable,
                "column_default": default,
                "column_description": desc,
            })

    fks_by_table: Dict[Tuple[str, str], List[Dict[str, str]]] = {}
    if foreign_keys:
        colmap = {name: idx for idx, name in enumerate(foreign_keys.columns)}
        for r in foreign_keys.rows:
            from_schema = safe_get(r, colmap.get("from_schema", 0))
            from_table = safe_get(r, colmap.get("from_table", 1))
            from_col = safe_get(r, colmap.get("from_column", 2))
            to_schema = safe_get(r, colmap.get("to_schema", 3))
            to_table = safe_get(r, colmap.get("to_table", 4))
            to_col = safe_get(r, colmap.get("to_column", 5))
            cname = safe_get(r, colmap.get("constraint_name", 6))
            fks_by_table.setdefault((from_schema, from_table), []).append({
                "from_column": from_col,
                "to_schema": to_schema,
                "to_table": to_table,
                "to_column": to_col,
                "constraint_name": cname,
            })

            # Also useful to have incoming refs (optional)
            fks_by_table.setdefault((to_schema, to_table), [])

            # FK chunk itself (row-level)
            fk_text = (
                f"Foreign key {cname}: "
                f"{from_schema}.{from_table}.{from_col} -> {to_schema}.{to_table}.{to_col}"
            )
            chunks.append((fk_text, {
                "chunk_type": "fk",
                "from_schema": from_schema,
                "from_table": from_table,
                "from_column": from_col,
                "to_schema": to_schema,
                "to_table": to_table,
                "to_column": to_col,
                "constraint_name": cname,
            }))

    # Determine all tables from TABLES section if present, else from columns map
    table_list: List[Tuple[str, str]] = []
    if tables:
        colmap = {name: idx for idx, name in enumerate(tables.columns)}
        for r in tables.rows:
            schema = safe_get(r, colmap.get("table_schema", 0))
            table = safe_get(r, colmap.get("table_name", 1))
            table_list.append((schema, table))
    else:
        table_list = sorted(cols_by_table.keys())

    # Build per-table chunks
    for (schema, table) in table_list:
        desc = table_desc.get((schema, table), "")

        # TABLE_COMMENT chunk
        if desc:
            chunks.append((f"Table {schema}.{table} description: {desc}", {
                "chunk_type": "table_comment",
                "schema_name": schema,
                "table_name": table,
            }))

        # COLUMN chunks (row-level)
        for c in cols_by_table.get((schema, table), []):
            col_name = c["column_name"]
            col_text = (
                f"Column {schema}.{table}.{col_name} "
                f"type={c['data_type']} nullable={c['is_nullable']} default={c['column_default'] or 'NULL'}"
            )
            if c["column_description"]:
                col_text += f" description={c['column_description']}"
            chunks.append((col_text, {
                "chunk_type": "column",
                "schema_name": schema,
                "table_name": table,
                "column_name": col_name,
            }))

        # TABLE_SUMMARY chunk (most important for retrieval)
        # Keep it compact: top N columns + all FKs
        cols = cols_by_table.get((schema, table), [])
        fk_list = fks_by_table.get((schema, table), [])

        top_cols = cols[:25]  # limit for token efficiency
        col_lines = []
        for c in top_cols:
            line = f"- {c['column_name']} ({c['data_type']})"
            if c["column_description"]:
                line += f": {c['column_description']}"
            col_lines.append(line)

        fk_lines = []
        for fk in fk_list[:30]:
            fk_lines.append(
                f"- {fk['constraint_name']}: {schema}.{table}.{fk['from_column']} -> "
                f"{fk['to_schema']}.{fk['to_table']}.{fk['to_column']}"
            )

        summary_parts = [
            f"TABLE {schema}.{table}",
            f"Description: {desc}" if desc else "Description: (none)",
            "Columns:",
            *(col_lines if col_lines else ["(no columns found)"]),
            "Foreign keys:",
            *(fk_lines if fk_lines else ["(no foreign keys found)"]),
        ]

        summary_text = "\n".join(summary_parts)

        chunks.append((summary_text, {
            "chunk_type": "table_summary",
            "schema_name": schema,
            "table_name": table,
        }))

    return chunks


# ---------- Vector store (Chroma) ----------

def save_to_chroma(
    chunks: List[Tuple[str, Dict[str, str]]],
    persist_dir: str = "./chroma_db",
    collection_name: str = "pg_schema",
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
) -> None:


    """
    Saves chunks into a persistent local Chroma store.
    """
    client = chromadb.PersistentClient(path=persist_dir, settings=Settings(anonymized_telemetry=False))
    collection = client.get_or_create_collection(name=collection_name)

    model = SentenceTransformer(embedding_model)

    texts = [t for (t, _) in chunks]
    metadatas = [m for (_, m) in chunks]
    ids = [str(uuid.uuid4()) for _ in chunks]

    embeddings = model.encode(texts, show_progress_bar=True, normalize_embeddings=True).tolist()

    # Upsert in batches to avoid large memory spikes
    batch_size = 50
    for start in range(0, len(texts), batch_size):
        end = start + batch_size
        collection.upsert(
            ids=ids[start:end],
            documents=texts[start:end],
            metadatas=metadatas[start:end],
            embeddings=embeddings[start:end],
        )